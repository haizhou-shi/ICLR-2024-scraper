{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d978f9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "chrome_path = ChromeDriverManager().install()\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def create_if_not_exists(path: str) -> None:\n",
    "    \"\"\"\n",
    "    Creates the specified folder if it does not exist.\n",
    "    :param path: the complete path of the folder to be created\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def extract_from_paper_elements(paper_elements, sleep_time=0.05):\n",
    "    results = []\n",
    "    for paper in paper_elements:\n",
    "        href = paper.find_element_by_tag_name('h4 > a').get_attribute('href')\n",
    "        paper.find_element_by_css_selector('div > div.collapse-widget > a').click()\n",
    "        #accept-oral > div > div > ul > li:nth-child(1) > div > div.collapse-widget > a\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "        contents = paper.text.split('\\n')\n",
    "        title, keywords, abstract = contents[0], None, None\n",
    "        for i, content in enumerate(contents):\n",
    "            if content.startswith('Keywords'):\n",
    "                keywords = [x.strip().lower() for x in content.split(':')[1].split(',')]\n",
    "            elif content.startswith('Abstract'):\n",
    "                start = i+1\n",
    "            elif content.startswith('Primary Area') or content.startswith('Code Of Ethics') or content.startswith('Supplementary'):\n",
    "                end = i\n",
    "                break\n",
    "        \n",
    "        abstract = ' '.join(contents[start:end])\n",
    "\n",
    "        results.append({\n",
    "            'title': title,\n",
    "            'href': href,\n",
    "            'keywords': keywords,\n",
    "            'abstract': abstract,\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def collect_all_papers(chrome_path, paper_type, num_pages, max_retries=10, sleep_time=0.05):\n",
    "    assert paper_type in ['oral', 'poster', 'spotlight']\n",
    "    session = webdriver.Chrome(chrome_path)\n",
    "    session.implicitly_wait(10)\n",
    "\n",
    "    session.get(f'https://openreview.net/group?id=ICLR.cc/2024/Conference&referrer=%5BHomepage%5D(%2F)#tab-accept-{paper_type}')\n",
    "    time.sleep(0.2)\n",
    "\n",
    "    papers = dict()\n",
    "\n",
    "    start = time.time()\n",
    "    for page in range(num_pages):\n",
    "        paper_elements = session.find_element_by_id(f'accept-{paper_type}').find_element_by_class_name('submissions-list').find_elements_by_css_selector('li > div')\n",
    "\n",
    "        for i in range(max_retries):\n",
    "            try:\n",
    "                results = extract_from_paper_elements(paper_elements, sleep_time=sleep_time)\n",
    "                break\n",
    "            except:\n",
    "                print(f'retrying ({i}/{max_retries})...')\n",
    "                time.sleep(sleep_time)\n",
    "        \n",
    "        # appending to the final result.\n",
    "        for res in results:\n",
    "            papers[res['href']] = res\n",
    "        \n",
    "        print(f'page {page+1}/{num_pages} done. ({time.time()-start:.2f} secs)')\n",
    "        start = time.time()\n",
    "        \n",
    "        if page == num_pages - 1:\n",
    "            return papers\n",
    "        \n",
    "        # Get the button and nav to the next page\n",
    "        button = session.find_element_by_css_selector(f'#accept-{paper_type} > div > div > nav > ul > li.right-arrow > a') # get the first right-arrow button.\n",
    "        button.click()\n",
    "        time.sleep(2*sleep_time)\n",
    "        session.execute_script(\"window.scrollTo(0, 0);\")\n",
    "        time.sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.abspath('../data/ICLR/2024/')\n",
    "create_if_not_exists(data_dir)\n",
    "\n",
    "orals = collect_all_papers(chrome_path, 'oral', num_pages=4, sleep_time=0.2)\n",
    "with open(os.path.join(data_dir, 'orals.json'), 'w') as f:\n",
    "    json.dump(orals, f, indent=4)\n",
    "\n",
    "spotlights = collect_all_papers(chrome_path, 'spotlight', num_pages=15, sleep_time=0.2)\n",
    "with open(os.path.join(data_dir, 'spotlights.json'), 'w') as f:\n",
    "    json.dump(spotlights, f, indent=4)\n",
    "\n",
    "posters = collect_all_papers(chrome_path, 'poster', num_pages=72, sleep_time=0.25)\n",
    "with open(os.path.join(data_dir, 'posters.json'), 'w') as f:\n",
    "    json.dump(posters, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
